{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>This notebook is designed to demonstrate how to connect to snowflake database using Spark and read the data from table into Spark dataframe</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Note:- If you are using ICP4D and need to whitelist the ip addresses at snowflake, you can get them using Hamburger menu -> Administration -> Cloud Integrations -> Firewall Configuration </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get spark version and scala version as we need to download the driver specific to this versions\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12\r\n"
     ]
    }
   ],
   "source": [
    "#Get scala version\n",
    "!echo $SPARK_SCALA_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the docs https://docs.snowflake.com/en/user-guide/spark-connector-install.html#step-4-configure-the-local-spark-cluster-or-amazon-emr-hosted-spark-environment\n",
    "#indicates that you need two drivers.\n",
    "\n",
    "# For Snowflake JDBC we need to use following jar hosted in maven repo https://mvnrepository.com/artifact/net.snowflake/snowflake-jdbc/3.12.17\n",
    "\n",
    "# For Snowflake Spark connector, we need to use following jar hosted in maven repo https://mvnrepository.com/artifact/net.snowflake/spark-snowflake_2.12/2.8.4-spark_3.0\n",
    "# Please look for another version based on the values you find above for your spark and scala version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-02 17:05:36--  https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.8.4-spark_3.0/spark-snowflake_2.12-2.8.4-spark_3.0.jar\r\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 199.232.8.209\r\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|199.232.8.209|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 710798 (694K) [application/java-archive]\r\n",
      "Server file no newer than local file ‘/home/spark/shared/user-libs/spark2/spark-snowflake_2.12-2.8.4-spark_3.0.jar’ -- not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -N https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.8.4-spark_3.0/spark-snowflake_2.12-2.8.4-spark_3.0.jar -P /home/spark/shared/user-libs/spark2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-02 17:05:38--  https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.12.17/snowflake-jdbc-3.12.17.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 199.232.8.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|199.232.8.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30823792 (29M) [application/java-archive]\n",
      "Server file no newer than local file ‘/home/spark/shared/user-libs/spark2/snowflake-jdbc-3.12.17.jar’ -- not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -N https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.12.17/snowflake-jdbc-3.12.17.jar -P /home/spark/shared/user-libs/spark2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20210402170558-0001\n",
      "KERNEL_ID = 17f930b0-d8f5-4c7e-a9b6-4e5b52684d5d\n"
     ]
    }
   ],
   "source": [
    "#We need to restart the kernel so will have new spark context that can avail the downloaded jar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add snowdlake jdbc and spark snowflake connector jars to spark context so that they are copied to executors.\n",
    "sc.addPyFile('/home/spark/shared/user-libs/spark2/spark-snowflake_2.12-2.8.4-spark_3.0.jar')\n",
    "sc.addPyFile('/home/spark/shared/user-libs/spark2/snowflake-jdbc-3.12.17.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment this cell and add your credentials to snowflake database\n",
    "#snowflake_credentials = {\n",
    "#    'username': 'chuck',\n",
    "#    'password': \"\"\"XXXXXXX\"\"\",\n",
    "#    'database': 'DEMO_DB',\n",
    "#    'port': '50000',\n",
    "#    'account_name': 'xxxxx',\n",
    "#    'warehouse': 'COMPUTE_WH',\n",
    "#    'role': 'ACCOUNTADMIN',\n",
    "#    'schema': 'PUBLIC'\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake options for Spark connector\n",
    "sf_opts_spark = {\n",
    "    \"sfURL\"       : snowflake_credentials['account_name']+\".snowflakecomputing.com\",  \n",
    "    \"sfRole\"      : snowflake_credentials['role'],\n",
    "    \"sfUser\"      : snowflake_credentials['username'],\n",
    "    \"sfPassword\"  : snowflake_credentials['password'],\n",
    "    \"sfDatabase\"  : snowflake_credentials['database'],\n",
    "    \"sfSchema\"    : snowflake_credentials['schema'],\n",
    "    \"sfWarehouse\" : snowflake_credentials['warehouse'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "query = \"\"\"\n",
    "select * from PUBLIC.CHUCKTEST limit 100\n",
    "\"\"\"\n",
    "df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sf_opts_spark).option(\"query\", query).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+--------------------+\n",
      "| ID|   NAME|RANK|                DATE|\n",
      "+---+-------+----+--------------------+\n",
      "|  1|charles|10.5|2021-04-02 16:59:...|\n",
      "|  2|    sam| 5.5|2021-04-02 16:59:...|\n",
      "|  3|  chloe| 2.8|2021-04-02 16:59:...|\n",
      "|  4|    Amy| 4.9|2021-04-02 16:59:...|\n",
      "+---+-------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 with Spark",
   "language": "python3",
   "name": "python37"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
